{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84994ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_edge_data = pd.read_csv(\"https://raw.githubusercontent.com/OpenFreeEnergy/IndustryBenchmarks2024/refs/heads/main/industry_benchmarks/analysis/processed_results/combined_pymbar3_edge_data.csv\")\n",
    "rerun_edge_data = pd.read_csv(\"https://raw.githubusercontent.com/OpenFreeEnergy/IndustryBenchmarks2024/refs/heads/main/industry_benchmarks/analysis/processed_results/reruns/rerun_pymbar3_edge_data.csv\")\n",
    "private_edge_data = pd.read_csv(\"https://raw.githubusercontent.com/OpenFreeEnergy/IndustryBenchmarks2024/refs/heads/main/industry_benchmarks/analysis/private_processed_results/combined_pymbar3_edge_data.csv\")\n",
    "private_edge_data = private_edge_data[private_edge_data[\"failed\"] != True]\n",
    "normal_edge_data = normal_edge_data[(normal_edge_data[\"system name\"] != \"pfkfb3\") & (normal_edge_data[\"failed\"] != True)]\n",
    "normal_edge_data = pd.concat([normal_edge_data, rerun_edge_data], ignore_index=True)\n",
    "normal_edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the public and private DG data and calculate the all-to-all pairwaise differences\n",
    "public_dg_data = pd.read_csv(\"https://raw.githubusercontent.com/OpenFreeEnergy/IndustryBenchmarks2024/refs/heads/main/industry_benchmarks/analysis/processed_results/combined_pymbar3_calculated_dg_data.csv\")\n",
    "rerun_dg_data = pd.read_csv(\"https://raw.githubusercontent.com/OpenFreeEnergy/IndustryBenchmarks2024/refs/heads/main/industry_benchmarks/analysis/processed_results/reruns/rerun_pymbar3_calculated_dg_data.csv\")\n",
    "private_dg_data = pd.read_csv(\"https://raw.githubusercontent.com/OpenFreeEnergy/IndustryBenchmarks2024/refs/heads/main/industry_benchmarks/analysis/private_processed_results/combined_pymbar3_calculated_dg_data.csv\")\n",
    "public_dg_data = public_dg_data[(public_dg_data[\"system name\"] != \"pfkfb3\") ]\n",
    "public_dg_data = pd.concat([public_dg_data, rerun_dg_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all pairwise differences for the public data and priavte data \n",
    "def calc_pairwise_differences(df, public=True):\n",
    "    if public:\n",
    "        group_name = \"system group\"\n",
    "        system_name = \"system name\"\n",
    "    else:\n",
    "        group_name = \"partner_id\"\n",
    "        system_name = \"dataset_name\"\n",
    "    pairwise_diffs = []\n",
    "    for system in df[group_name].unique():\n",
    "        system_df = df[df[group_name] == system].copy(deep=True).reset_index(drop=True)\n",
    "        for target in system_df[system_name].unique():\n",
    "            target_df = system_df[system_df[system_name] == target].copy(deep=True).reset_index(drop=True)\n",
    "            # get a list of unique ligand names\n",
    "            ligands = target_df[\"ligand name\"].unique()\n",
    "            for i, ligand1 in enumerate(ligands):\n",
    "                for j, ligand2 in enumerate(ligands):\n",
    "                    if i >= j:  # skip self-comparisons\n",
    "                        continue\n",
    "                    # get the ddg values for these ligands\n",
    "                    exp_dg1 = target_df[target_df[\"ligand name\"] == ligand1][\"Exp DG (kcal/mol)\"].values[0]\n",
    "                    exp_dg2 = target_df[target_df[\"ligand name\"] == ligand2][\"Exp DG (kcal/mol)\"].values[0]\n",
    "                    openfe_dg1 = target_df[target_df[\"ligand name\"] == ligand1][\"DG (kcal/mol)\"].values[0]\n",
    "                    openfe_dg2 = target_df[target_df[\"ligand name\"] == ligand2][\"DG (kcal/mol)\"].values[0]\n",
    "                    # add to a new dataframe\n",
    "                    new_row = {\n",
    "                        \"System\": target,\n",
    "                        \"Class\": system,\n",
    "                        \"Ligand 1\": ligand1,\n",
    "                        \"Ligand 2\": ligand2,\n",
    "                        \"Exp DDG (kcal/mol)\": exp_dg2 - exp_dg1,\n",
    "                        \"OpenFE DDG (kcal/mol)\": openfe_dg2 - openfe_dg1,\n",
    "                    }\n",
    "                    pairwise_diffs.append(new_row)\n",
    "    return pd.DataFrame(pairwise_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab321a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_pairwise_diffs = calc_pairwise_differences(public_dg_data)\n",
    "private_pairwise_diffs = calc_pairwise_differences(private_dg_data, public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_system_pairwise_rmse(df):\n",
    "    rmse_data = []\n",
    "    for system in df[\"Class\"].unique():\n",
    "        system_df = df[df[\"Class\"] == system].copy(deep=True).reset_index(drop=True)\n",
    "        targets = system_df[\"System\"].unique()\n",
    "        for target in targets:\n",
    "            target_df = system_df[system_df[\"System\"] == target].copy(deep=True).reset_index(drop=True)\n",
    "            \n",
    "            rmse = ((target_df[\"OpenFE DDG (kcal/mol)\"] - target_df[\"Exp DDG (kcal/mol)\"]) ** 2).mean() ** 0.5\n",
    "            mue = np.abs(target_df[\"OpenFE DDG (kcal/mol)\"] - target_df[\"Exp DDG (kcal/mol)\"]).mean()\n",
    "            # bootstrap the RMSE\n",
    "            n_bootstraps = 1000\n",
    "            bootstrapped_rmse = np.zeros(n_bootstraps)\n",
    "            bootstapped_mue = np.zeros(n_bootstraps)\n",
    "            for i in range(n_bootstraps):\n",
    "                sample_df = target_df.sample(frac=1, replace=True)\n",
    "                bootstrapped_rmse[i] = ((sample_df[\"OpenFE DDG (kcal/mol)\"] - sample_df[\"Exp DDG (kcal/mol)\"]) ** 2).mean() ** 0.5\n",
    "                bootstapped_mue[i] = np.abs(sample_df[\"OpenFE DDG (kcal/mol)\"] - sample_df[\"Exp DDG (kcal/mol)\"]).mean()\n",
    "            lower_bound = np.percentile(bootstrapped_rmse, 2.5)\n",
    "            upper_bound = np.percentile(bootstrapped_rmse, 97.5)\n",
    "            rmse_data.append({\"System\": target, \"RMSE (kcal/mol)\": rmse, \n",
    "            \"RMSE lower\": lower_bound, \"RMSE upper\": upper_bound, \"Class\": system, \n",
    "            \"n ligands\": len(target_df[\"Ligand 1\"].unique())\n",
    "            \"MUE\": mue, \"MUE lower\": np.percentile(bootstapped_mue, 2.5),\n",
    "            \"MUE upper\": np.percentile(bootstapped_mue, 97.5)})\n",
    "    # now calculate the weighted RMSE and add it to the end of the dataframe\n",
    "    rmse_df = pd.DataFrame(rmse_data)\n",
    "    weighted_rmse = np.sqrt(np.sum(rmse_df[\"RMSE (kcal/mol)\"] ** 2 * rmse_df[\"n ligands\"]) / rmse_df[\"n ligands\"].sum())\n",
    "    weighted_mue = \n",
    "    # bootstrap the weighted RMSE\n",
    "    bootstrapped_weighted_rmse = np.zeros(n_bootstraps)\n",
    "    for i in range(n_bootstraps):\n",
    "        sample_df = rmse_df.sample(frac=1, replace=True)\n",
    "        bootstrapped_weighted_rmse[i] = np.sqrt(np.sum(sample_df[\"RMSE (kcal/mol)\"] ** 2 * sample_df[\"n ligands\"]) / sample_df[\"n ligands\"].sum())\n",
    "    lower_bound_weighted = np.percentile(bootstrapped_weighted_rmse, 2.5)\n",
    "    upper_bound_weighted = np.percentile(bootstrapped_weighted_rmse, 97.5)\n",
    "    # sort the dataframe by RMSE\n",
    "    rmse_df.sort_values(by=[\"Class\", \"RMSE (kcal/mol)\"], inplace=True)\n",
    "    # concatenate the weighted RMSE to the end of the dataframe\n",
    "    row_data = {\n",
    "        \"System\": \"Weighted\\nRMSE\",\n",
    "        \"RMSE (kcal/mol)\": weighted_rmse,\n",
    "        \"RMSE lower\": lower_bound_weighted,\n",
    "        \"RMSE upper\": upper_bound_weighted,\n",
    "        \"Class\": \"Overall\",\n",
    "        \"n ligands\": rmse_df[\"n ligands\"].sum()\n",
    "    }\n",
    "    rmse_data = pd.concat([rmse_df, pd.DataFrame([row_data])], ignore_index=True)\n",
    "    return rmse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_pairwise_rmse_private = calculate_system_pairwise_rmse(private_pairwise_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61368e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_pairwise_rmse_private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de80507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pairwise RMSEs for the private datasets\n",
    "import numpy as np\n",
    "x = np.arange(len(system_pairwise_rmse_private))\n",
    "\n",
    "# Set up the figure\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "bar_width = 0.6\n",
    "\n",
    "# Plot bars with error bars\n",
    "rmse_err_openfe = [system_pairwise_rmse_private['RMSE (kcal/mol)'] - system_pairwise_rmse_private['RMSE lower'], system_pairwise_rmse_private['RMSE upper'] - system_pairwise_rmse_private['RMSE (kcal/mol)']]\n",
    "# \"OpenFE\": \"#009384\", \"FEP+\": \"#d9c4b1\"\n",
    "ax.bar(x, system_pairwise_rmse_private['RMSE (kcal/mol)'], yerr=rmse_err_openfe, width=bar_width, label='OpenFE',\n",
    "       color='#009384', capsize=3)\n",
    "\n",
    "# Set labels and ticks\n",
    "ax.set_xticks(x)\n",
    "names = system_pairwise_rmse_private['System'].str.replace(\"_\", \" \")\n",
    "ax.set_xticklabels(names, rotation=90, fontsize=8)\n",
    "ax.set_ylabel(r\"Pairwise $\\Delta\\Delta$G$_{calc}$ RMSE (kcal/mol)\", fontsize=12)\n",
    "\n",
    "\n",
    "unique_classes = system_pairwise_rmse_private['Class'].unique()\n",
    "# # remove overall from the unique classes and add it to the end\n",
    "unique_classes = [cls for cls in unique_classes if cls != \"Overall\"]\n",
    "unique_classes.append(\"Overall\")\n",
    "class_bounds = system_pairwise_rmse_private.groupby('Class').size().cumsum().to_dict()\n",
    "# # move overall to the end\n",
    "class_bounds[\"Overall\"] = len(system_pairwise_rmse_private) + 1\n",
    "\n",
    "#Â change the roche end limit\n",
    "class_bounds[\"Roche\"] -= 1\n",
    "print(class_bounds)\n",
    "class_start = 0\n",
    "\n",
    "colors = sns.color_palette(\"colorblind\", len(unique_classes))\n",
    "class_conversion = {\"bayer_macrocycles\": \"Bayer\\nMacrocycles\", \"charge_annihilation_set\": \"Charge\\nAnnihilation\", \"fragments\": \"Fragments\", \"jacs_set\": \"JACS\", \"janssen_bace\": \"Janssen\", \"merck\": \"Merck\", \"miscellaneous_set\": \"Misc\", \"scaffold_hopping_set\": \"Scaffold\\nHopping\", \"water_set\": \"Water\", \"mcs_docking_set\": \"MCS\\nDocking\"}\n",
    "for i, cls in enumerate(unique_classes):\n",
    "    cls_name = class_conversion.get(cls, cls)\n",
    "    end = class_bounds[cls]\n",
    "    # add dashed lines for the span but not the fill\n",
    "    if cls != \"ASAP\":\n",
    "        ax.axvline(class_start - 0.5, linestyle='--', linewidth=2)\n",
    "#     ax.axvspan(class_start - 0.5, end - 0.5, facecolor=colors[i], alpha=0.2)\n",
    "    center = (class_start + end - 1) / 2\n",
    "    ax.text(center, ax.get_ylim()[1] + 0.1, cls_name, ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "    class_start = end\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim(-0.5, len(system_pairwise_rmse_private) - 0.5)\n",
    "plt.savefig(\"per_system_pairwise_rmse_private.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f6d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ecdfs of the pairwise differences for openfe\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# plot the ecdf for the public data\n",
    "# calculate the abs errors\n",
    "private_error = np.abs(private_pairwise_diffs[\"OpenFE DDG (kcal/mol)\"] - private_pairwise_diffs[\"Exp DDG (kcal/mol)\"])\n",
    "sns.ecdfplot(private_error, label=\"Private\", ax=ax, color='#009384')\n",
    "# workout the percentage of values below 1 kcal/mol on the private data\n",
    "private_below_1 = np.sum(private_error < 1) / len(private_error) \n",
    "# add a line at 1 kcal/mol for private\n",
    "ax.axvline(x=1, ymax=private_below_1, color='k', linestyle='--', linewidth=2)\n",
    "ax.plot([0, 1], [private_below_1, private_below_1], color='#009384', linestyle='--', linewidth=2)\n",
    "# add the text for the percentage below 1 kcal/mol\n",
    "ax.text(0.1, private_below_1 + 0.02, f\"{private_below_1:.2%}\", color='#009384', fontsize=13, zorder=10)\n",
    "# do the same for 2 kcal/mol\n",
    "private_below_2 = np.sum(private_error < 2) / len(private_error)\n",
    "ax.axvline(x=2, ymax=private_below_2, color='k', linestyle='--', linewidth=2)\n",
    "ax.plot([0, 2], [private_below_2, private_below_2], color='#009384', linestyle='--', linewidth=2)\n",
    "# add the text for the percentage below 2 kcal/mol\n",
    "ax.text(0.1, private_below_2 + 0.02, f\"{private_below_2:.2%}\", color='#009384', fontsize=13)\n",
    "# same again at 3 kcal/mol\n",
    "private_below_3 = np.sum(private_error < 3) / len(private_error)\n",
    "ax.axvline(x=3, ymax=private_below_3, color='k', linestyle='--', linewidth=2)\n",
    "ax.plot([0, 3], [private_below_3, private_below_3], color='#009384', linestyle='--', linewidth=2)\n",
    "ax.text(0.1, private_below_3 + 0.02, f\"{private_below_3:.2%}\", color='#009384', fontsize=13)\n",
    "# plot the ecdf for the private data\n",
    "# set the labels and title\n",
    "ax.set_xlabel(r\"Pairwise $|\\Delta\\Delta$G$_{calc}-\\Delta\\Delta$G$_{exp}|$ (kcal/mol)\", fontdict={\"fontsize\": 15})\n",
    "ax.set_ylabel(\"Cumulative Probability\", fontdict={\"fontsize\": 15})\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "plt.xlim(left=0)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ecdf_pairwise_differences_private.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_error.sort_values(inplace=True, ascending=False)\n",
    "private_error.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f820378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability of the correct sign for the pairwise differences for the private data\n",
    "def bin_sign_correctness(data, bin_size=1.0):\n",
    "    \"\"\"\n",
    "    Bin the absolute experimental DDG values and calculate the probability of getting the sign correct.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with 'Exp DDG (kcal/mol)', 'OpenFE DDG (kcal/mol)', 'FEP+ DDG (kcal/mol)' columns.\n",
    "    - bin_size: Size of the bins for absolute DDG values.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with bins and probabilities for OpenFE and FEP+.\n",
    "    \"\"\"\n",
    "    # Create bins\n",
    "    data['abs_exp_ddg'] = np.abs(data['Exp DDG (kcal/mol)'])\n",
    "    # max value\n",
    "    max_value = data['abs_exp_ddg'].max()\n",
    "    bins = np.arange(0, max_value + bin_size, bin_size)\n",
    "    \n",
    "    # Bin the data\n",
    "    data['bin'] = pd.cut(data['abs_exp_ddg'], bins=bins, right=False)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    results = []\n",
    "    # sort the bins by the lower edge\n",
    "    for b in sorted(data['bin'].unique(), key=lambda x: x.left):\n",
    "        subset = data[data['bin'] == b]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        # Calculate the probability of correct sign for OpenFE and FEP+ and bootstrap the results\n",
    "        openfe_correct = np.sum(np.sign(subset['OpenFE DDG (kcal/mol)']) == np.sign(subset['Exp DDG (kcal/mol)']))\n",
    "        total = len(subset)\n",
    "        # Bootstrap the probabilities\n",
    "        nboots = 1000\n",
    "        openfe_probs = []\n",
    "        for _ in range(nboots):\n",
    "            bootstrapped_subset = subset.sample(n=len(subset), replace=True)\n",
    "            openfe_correct_boot = np.sum(np.sign(bootstrapped_subset['OpenFE DDG (kcal/mol)']) == np.sign(bootstrapped_subset['Exp DDG (kcal/mol)']))\n",
    "            total_boot = len(bootstrapped_subset)\n",
    "            openfe_probs.append(openfe_correct_boot / total_boot)\n",
    "        # Calculate mean and 95% CI\n",
    "        openfe_mean = np.mean(openfe_probs)\n",
    "        openfe_low = np.percentile(openfe_probs, 2.5)\n",
    "        openfe_high = np.percentile(openfe_probs, 97.5)\n",
    "        # store the results so we can use hue to split the data\n",
    "        results.append({\n",
    "            'bin': b.left,\n",
    "            'OpenFE Probability': openfe_mean,\n",
    "            'OpenFE Probability Lower': openfe_low,\n",
    "            'OpenFE Probability Upper': openfe_high,\n",
    "        })\n",
    "        # x+=1\n",
    "    return pd.DataFrame(results)\n",
    "# Calculate the binned probabilities\n",
    "binned_probabilities = bin_sign_correctness(private_pairwise_diffs, bin_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the binned probabilities using a bar plot with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "binned = binned_probabilities.copy(deep=True)\n",
    "# add padding data for the missing bins\n",
    "all_bins = binned['bin'].unique()\n",
    "# sort the bins\n",
    "sorted_bins = np.sort(all_bins)\n",
    "new_bins = np.arange(0, sorted_bins.max() + 0.5, 0.5)\n",
    "data_to_add = []\n",
    "for b in new_bins:\n",
    "    if b not in sorted_bins:\n",
    "        data_to_add.append({\n",
    "            'bin': b,\n",
    "            'OpenFE Probability': np.nan,\n",
    "            'OpenFE Probability Lower': np.nan,\n",
    "            'OpenFE Probability Upper': np.nan\n",
    "        })\n",
    "binned_probabilities = pd.concat([binned_probabilities, pd.DataFrame(data_to_add)], ignore_index=True)\n",
    "sns.barplot(data=binned_probabilities, x='bin', y='OpenFE Probability', ax=ax, color='#009384', width=1.0)\n",
    "# add error bars for the probabilities\n",
    "ax.set_xlabel(r\"|$\\Delta\\Delta$G$_{exp}$| (kcal/mol)\", fontsize=14)\n",
    "ax.set_ylabel(r\"Probability Correct pairwise $\\Delta\\Delta$G$_{calc}$ Sign\", fontsize=14)\n",
    "# set the x ticks to go from 0 to the max bin value in 1 kcal/mol increments\n",
    "x_ticks = np.arange(1.5, 20.5, 2)\n",
    "ax.errorbar(binned_probabilities['bin'] *2, binned_probabilities['OpenFE Probability'],\n",
    "            yerr=[binned_probabilities['OpenFE Probability'] - binned_probabilities['OpenFE Probability Lower'],\n",
    "                  binned_probabilities['OpenFE Probability Upper'] - binned_probabilities['OpenFE Probability']],\n",
    "            fmt='none', color='black', capsize=5)\n",
    "# add scatter points for the probabilities\n",
    "ax.scatter(binned_probabilities['bin'] *2, binned_probabilities['OpenFE Probability'], color='#009384', edgecolor='black', s=50)\n",
    "print(x_ticks)\n",
    "print(ax.get_xticks())\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], fontsize=12)\n",
    "# plt.xlim((-0.5,14.5))\n",
    "# # save the figure\n",
    "# plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.ylim(bottom=0.4)\n",
    "plt.xlim(left=-0.5)\n",
    "plt.savefig(\"binned_probabilities_private.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47167f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the inter repeat range in the DDG predictions as an ecdf\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for dataset, label in [(normal_edge_data, \"Public\"), (private_edge_data, \"Private\")]:\n",
    "    all_ranges = []\n",
    "    good_overlap = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        data = []\n",
    "        # get the overlaps for the solvent and complex repeats\n",
    "        overlaps = [row[f\"solvent_repeat_{i}_smallest_overlap\"] for i in range(3)]\n",
    "        for i in range(3):\n",
    "            overlaps.append(row[f\"complex_repeat_{i}_smallest_overlap\"])\n",
    "\n",
    "        # calculate the ddg for each repeat\n",
    "        for i in range(3):\n",
    "            ddg = row[f\"complex_repeat_{i}_DG (kcal/mol)\"] - row[f\"solvent_repeat_{i}_DG (kcal/mol)\"]\n",
    "            data.append(ddg)\n",
    "\n",
    "        ddg_range = max(data) - min(data)\n",
    "        # append the range to the all_ranges list\n",
    "        all_ranges.append(ddg_range)\n",
    "        # if the overlaps are all greater than 0.03, append the ddg_range to the good_overlap list\n",
    "        if min(overlaps) >= 0.03:\n",
    "            good_overlap.append(max(data) - min(data))\n",
    "\n",
    "    # workout the percentage of edges with a repeat range less than 1 kcal/mol\n",
    "    all_ranges = np.array(all_ranges)\n",
    "    good_overlap = np.array(good_overlap)\n",
    "    below_1 = np.sum(all_ranges < 1) / len(all_ranges)\n",
    "    below_1_overlap = np.sum(good_overlap < 1) / len(good_overlap)\n",
    "    print(f\"{label} - Percentage of edges with repeat range < 1 kcal/mol: {below_1:.2%}\")\n",
    "    print(f\"{label} - Percentage of edges with repeat range < 1 kcal/mol and overlap > 0.03: {below_1_overlap:.2%}\")\n",
    "    # remove the outlier values from the all_ranges\n",
    "    colour = sns.color_palette()[0] if label == \"Public\" else sns.color_palette()[1]\n",
    "    all_ranges = [x for x in all_ranges if x < 300]  # filter out values greater than 300\n",
    "    sns.ecdfplot(all_ranges, ax=ax, label=f\"{label}-all\", linewidth=2, color=colour)\n",
    "    # plot with the same colour but dash line for good overlap\n",
    "    good_overlap = [x for x in good_overlap if x < 300]  # filter out values greater than 300\n",
    "    sns.ecdfplot(good_overlap, ax=ax, label=f\"{label}-overlap > 0.03\", linestyle='--', linewidth=2, color=colour)\n",
    "ax.set_xlabel(r\"|$\\Delta\\Delta$G$_{calc}$| repeat range (kcal/mol)\", fontsize=12)\n",
    "ax.set_ylabel(\"Cumulative Probability\", fontsize=12)\n",
    "# set the axis ticks fontsize\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.savefig(\"ddg_repeat_range_ecdf.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the inter repeat range and the smallest overlap to the public and private edge data and sort by the inter repeat range\n",
    "def add_inter_repeat_range_and_overlap(edge_data):\n",
    "    inter_repeat_ranges = []\n",
    "    smallest_overlaps = []\n",
    "    for _, row in edge_data.iterrows():\n",
    "        data = []\n",
    "        # get the overlaps for the solvent and complex repeats\n",
    "        overlaps = [row[f\"solvent_repeat_{i}_smallest_overlap\"] for i in range(3)]\n",
    "        for i in range(3):\n",
    "            overlaps.append(row[f\"complex_repeat_{i}_smallest_overlap\"])\n",
    "\n",
    "        # calculate the ddg for each repeat\n",
    "        for i in range(3):\n",
    "            ddg = row[f\"complex_repeat_{i}_DG (kcal/mol)\"] - row[f\"solvent_repeat_{i}_DG (kcal/mol)\"]\n",
    "            data.append(ddg)\n",
    "\n",
    "        ddg_range = max(data) - min(data)\n",
    "        inter_repeat_ranges.append(ddg_range)\n",
    "        smallest_overlaps.append(min(overlaps))\n",
    "\n",
    "    edge_data[\"inter_repeat_range\"] = inter_repeat_ranges\n",
    "    edge_data[\"smallest_overlap\"] = smallest_overlaps\n",
    "    return edge_data\n",
    "# Add the inter repeat range and smallest overlap to the edge data\n",
    "normal_edge_data = add_inter_repeat_range_and_overlap(normal_edge_data)\n",
    "private_edge_data = add_inter_repeat_range_and_overlap(private_edge_data)\n",
    "# Sort the edge data by the inter repeat range\n",
    "normal_edge_data.sort_values(by=\"inter_repeat_range\", inplace=True, ascending=False)\n",
    "private_edge_data.sort_values(by=\"inter_repeat_range\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c656cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_edge_data.iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each repeat and the average calculate the RMSE MUE and Ktau for the DG values and the DDG edgewise and pairwise errors \n",
    "from collections import defaultdict\n",
    "from cinnabar import FEMap\n",
    "from cinnabar.stats import bootstrap_statistic\n",
    "from openff.units import unit\n",
    "def calculate_dgs(edge_df, dg_df, public=True) -> tuple[pd.DataFrame]:\n",
    "    if public:\n",
    "        group_name = \"system group\"\n",
    "        system_name = \"system name\"\n",
    "    else:\n",
    "        group_name = \"partner_id\"\n",
    "        system_name = \"dataset_name\"\n",
    "    calculated_dgs = []\n",
    "    for system in edge_df[group_name].unique():\n",
    "        # get the edges for this system\n",
    "        system_df = edge_df[edge_df[group_name] == system].copy(deep=True).reset_index(drop=True)\n",
    "        targets = system_df[system_name].unique()\n",
    "        for target in targets:\n",
    "            print(f\"Calculating DGs for {target} in {system}\")\n",
    "            # get the edges for this target\n",
    "            target_df = system_df[(system_df[system_name] == target)].copy(deep=True).reset_index(drop=True)\n",
    "            exp_target_data = dg_df[(dg_df[system_name] == target) & (dg_df[group_name] == system)].copy(deep=True).reset_index(drop=True)\n",
    "            exp_shift = exp_target_data[\"Exp DG (kcal/mol)\"].mean()  # shift the experimental values to match the OpenFE values\n",
    "            # calculate the stats for each repeat\n",
    "            target_data = {}\n",
    "            for i in range(3):\n",
    "\n",
    "                # calculate the absolute DG values\n",
    "                fe_map = FEMap()\n",
    "                for _, row in target_df.iterrows():\n",
    "                    complex_dg = row[f\"complex_repeat_{i}_DG (kcal/mol)\"]\n",
    "                    complex_error = row[f\"complex_repeat_{i}_dDG (kcal/mol)\"]\n",
    "                    solvent_dg = row[f\"solvent_repeat_{i}_DG (kcal/mol)\"]\n",
    "                    solvent_error = row[f\"solvent_repeat_{i}_dDG (kcal/mol)\"]\n",
    "                    uncertainty = (complex_error**2 + solvent_error**2)**0.5 * unit.kilocalorie_per_mole\n",
    "                    if uncertainty < 0.01 * unit.kilocalorie_per_mole:\n",
    "                        uncertainty = 0.1 * unit.kilocalorie_per_mole\n",
    "                    fe_map.add_relative_calculation(\n",
    "                        value=(complex_dg - solvent_dg) * unit.kilocalorie_per_mole,\n",
    "                        uncertainty=uncertainty if np.isfinite(uncertainty) else 0.1 * unit.kilocalorie_per_mole,\n",
    "                        labelA=row[\"ligand_A\"],\n",
    "                        labelB=row[\"ligand_B\"],\n",
    "                    )\n",
    "                # calculate the absolute DG values\n",
    "                fe_map.generate_absolute_values()\n",
    "                # get the absolute DG values\n",
    "                abs_df = fe_map.get_absolute_dataframe()\n",
    "                # write them to the target data\n",
    "                for _, abs_row in abs_df.iterrows():\n",
    "                    if abs_row[\"label\"] not in target_data:\n",
    "                        target_data[abs_row[\"label\"]] = {\"system group\": system, \"system name\": target, \"ligand name\": abs_row[\"label\"]}\n",
    "                        # add the exp data and the average calculated dg\n",
    "                        try:\n",
    "                            avg_data = exp_target_data[exp_target_data[\"ligand name\"] == abs_row[\"label\"]].iloc[0]\n",
    "                        except IndexError as e:\n",
    "                                print(abs_row[\"label\"], \"not found in experimental data\")\n",
    "                                continue\n",
    "                        target_data[abs_row[\"label\"]][\"exp DG (kcal/mol)\"] = avg_data[\"Exp DG (kcal/mol)\"]\n",
    "                        target_data[abs_row[\"label\"]][\"exp dDG (kcal/mol)\"] = avg_data[\"Exp dDG (kcal/mol)\"]\n",
    "                        target_data[abs_row[\"label\"]][\"average DG (kcal/mol)\"] = avg_data[\"DG (kcal/mol)\"]\n",
    "                        target_data[abs_row[\"label\"]][\"average dDG (kcal/mol)\"] = avg_data[\"uncertainty (kcal/mol)\"]\n",
    "                    target_data[abs_row[\"label\"]][f\"repeat_{i}_DG (kcal/mol)\"] = abs_row[\"DG (kcal/mol)\"] + exp_shift\n",
    "                    # calculate the uncertainty\n",
    "                    target_data[abs_row[\"label\"]][f\"repeat_{i}_dDG (kcal/mol)\"] = abs_row[\"uncertainty (kcal/mol)\"]\n",
    "            calculated_dgs.extend(list(target_data.values()))\n",
    "    # create a new dataframe with all of the metrics calculated for each system\n",
    "    calculated_dgs = pd.DataFrame(calculated_dgs)\n",
    "    return calculated_dgs\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_dg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_repeat_dgs_public = calculate_dgs(normal_edge_data, public_dg_data, public=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae631f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the all-to-all pairwise differences for the public data for each repeat and the average in a new df\n",
    "def calculate_pairwise_diffs(dg_df, public=True) -> pd.DataFrame:\n",
    "    if public:\n",
    "        group_name = \"system group\"\n",
    "        system_name = \"system name\"\n",
    "    else:\n",
    "        group_name = \"partner_id\"\n",
    "        system_name = \"dataset_name\"\n",
    "    pairwise_diffs = []\n",
    "    for system in dg_df[group_name].unique():\n",
    "        system_df = dg_df[dg_df[group_name] == system].copy(deep=True).reset_index(drop=True)\n",
    "        targets = system_df[system_name].unique()\n",
    "        for target in targets:\n",
    "            target_df = system_df[(system_df[system_name] == target)].copy(deep=True).reset_index(drop=True)\n",
    "            ligands = target_df[\"ligand name\"].unique()\n",
    "            for i, ligand1 in enumerate(ligands):\n",
    "                for j, ligand2 in enumerate(ligands):\n",
    "                    if i == j:  # skip self-comparisons\n",
    "                        continue\n",
    "                    # get the ddg values for these ligands\n",
    "                    exp_dg1 = target_df[target_df[\"ligand name\"] == ligand1][\"exp DG (kcal/mol)\"].values[0]\n",
    "                    exp_dg2 = target_df[target_df[\"ligand name\"] == ligand2][\"exp DG (kcal/mol)\"].values[0]\n",
    "                    openfe_dg1 = target_df[target_df[\"ligand name\"] == ligand1][\"average DG (kcal/mol)\"].values[0]\n",
    "                    openfe_dg2 = target_df[target_df[\"ligand name\"] == ligand2][\"average DG (kcal/mol)\"].values[0]\n",
    "                    openfe_error1 = target_df[target_df[\"ligand name\"] == ligand1][\"average dDG (kcal/mol)\"].values[0]\n",
    "                    openfe_error2 = target_df[target_df[\"ligand name\"] == ligand2][\"average dDG (kcal/mol)\"].values[0]\n",
    "                    # add to a new dataframe\n",
    "                    new_row = {\n",
    "                        \"system group\": system,\n",
    "                        \"system name\": target,\n",
    "                        \"Ligand 1\": ligand1,\n",
    "                        \"Ligand 2\": ligand2,\n",
    "                        \"Exp DDG (kcal/mol)\": exp_dg2 - exp_dg1,\n",
    "                        \"average DDG (kcal/mol)\": openfe_dg2 - openfe_dg1,\n",
    "                        \"average DDG uncertainty (kcal/mol)\": (openfe_error1**2 + openfe_error2**2)**0.5,\n",
    "                    }\n",
    "                    for i in range(3):\n",
    "                        repeat_dg1 = target_df[target_df[\"ligand name\"] == ligand1][f\"repeat_{i}_DG (kcal/mol)\"].values[0]\n",
    "                        repeat_dg2 = target_df[target_df[\"ligand name\"] == ligand2][f\"repeat_{i}_DG (kcal/mol)\"].values[0]\n",
    "                        repeat_error1 = target_df[target_df[\"ligand name\"] == ligand1][f\"repeat_{i}_dDG (kcal/mol)\"].values[0]\n",
    "                        repeat_error2 = target_df[target_df[\"ligand name\"] == ligand2][f\"repeat_{i}_dDG (kcal/mol)\"].values[0]\n",
    "                        new_row[f\"repeat_{i} DDG (kcal/mol)\"] = repeat_dg2 - repeat_dg1\n",
    "                        new_row[f\"repeat_{i} DDG uncertainty (kcal/mol)\"] = (repeat_error1**2 + repeat_error2**2)**0.5\n",
    "                    pairwise_diffs.append(new_row)\n",
    "    return pd.DataFrame(pairwise_diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_pariwise_diffs = calculate_pairwise_diffs(per_repeat_dgs_public, public=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_pariwise_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for the private data\n",
    "per_repeat_dgs_private = calculate_dgs(private_edge_data, private_dg_data, public=False)\n",
    "private_pairwise_diffs = calculate_pairwise_diffs(per_repeat_dgs_private, public=True)\n",
    "private_pairwise_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ecdf of the pairwise errors for each repeat and the average\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "colors = sns.color_palette(\"rocket\", 4)\n",
    "for dataset, label in [(public_pariwise_diffs, \"Public\"), (private_pairwise_diffs, \"Private\")]:\n",
    "    for i in range(3):\n",
    "        # calculate the absolute errors for each repeat\n",
    "        abs_error = np.abs(dataset[f\"repeat_{i} DDG (kcal/mol)\"] - dataset[\"Exp DDG (kcal/mol)\"])\n",
    "        sns.ecdfplot(abs_error, ax=ax, label=f\"Repeat {i+1} {label}\", linewidth=2, color=colors[i], linestyle='-' if label == \"Public\" else '--')\n",
    "    average_errors = np.abs(dataset[\"average DDG (kcal/mol)\"] - dataset[\"Exp DDG (kcal/mol)\"])\n",
    "    sns.ecdfplot(average_errors, ax=ax, label=f\"Average {label}\", linewidth=2, linestyle='-' if label == \"Public\" else '--', color=colors[3])\n",
    "ax.set_xlabel(r\"Pairwise |$\\Delta\\Delta$G$_{calc} - \\Delta\\Delta$G$_{exp}$| (kcal/mol)\", fontsize=14)\n",
    "ax.set_ylabel(\"Cumulative Probability\", fontsize=14)\n",
    "# simplify the legend to show colours for each repeat and average and the line style for public and private\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlim((0, 5))\n",
    "plt.savefig(\"pairwise_ddg_ecdf_zoom.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE and MUE statistics for the pairwise errors and create a summary table\n",
    "pair_wise_stats = []\n",
    "for dataset, label in [(public_pariwise_diffs, \"Public\"), (private_pairwise_diffs, \"Private\")]:\n",
    "\n",
    "    for i in range(3):\n",
    "        print(f\"Calculating statistics for repeat {i+1} in {label}\")\n",
    "        # only check edges that have a DDG value for this repeat\n",
    "        temp_ds = dataset[(dataset[f\"repeat_{i} DDG (kcal/mol)\"].notna()) & (dataset[\"Exp DDG (kcal/mol)\"].notna())].copy(deep=True).reset_index(drop=True)\n",
    "        repeat_data = {\"repeat\": i+1, \"dataset\": label}\n",
    "        exp_ddg = temp_ds[\"Exp DDG (kcal/mol)\"].values\n",
    "        openfe_ddg = temp_ds[f\"repeat_{i} DDG (kcal/mol)\"].values\n",
    "        mue, rmse = np.zeros(1000), np.zeros(1000)\n",
    "        for i in range(1000):\n",
    "            subset = np.random.choice(len(exp_ddg), size=len(exp_ddg), replace=True)\n",
    "            mue[i] = np.mean(np.abs(openfe_ddg[subset] - exp_ddg[subset]))\n",
    "            rmse[i] = np.sqrt(np.mean((openfe_ddg[subset] - exp_ddg[subset])**2))\n",
    "\n",
    "        repeat_data[\"RMSE\"] = np.mean(rmse)\n",
    "        repeat_data[\"RMSE lower\"] = np.percentile(rmse, 2.5)\n",
    "        repeat_data[\"RMSE upper\"] = np.percentile(rmse, 97.5)\n",
    "        repeat_data[\"MUE\"] = np.mean(mue)\n",
    "        repeat_data[\"MUE lower\"] = np.percentile(mue, 2.5)\n",
    "        repeat_data[\"MUE upper\"] = np.percentile(mue, 97.5)\n",
    "        pair_wise_stats.append(repeat_data)\n",
    "\n",
    "    # do the same for the average\n",
    "    temp_ds = dataset[(dataset[\"Exp DDG (kcal/mol)\"].notna())].copy(deep=True).reset_index(drop=True)\n",
    "\n",
    "    average_data = {\"repeat\": \"average\", \"dataset\": label}\n",
    "    exp_ddg = temp_ds[\"Exp DDG (kcal/mol)\"].values\n",
    "    openfe_ddg = temp_ds[\"average DDG (kcal/mol)\"].values\n",
    "    print(f\"Calculating statistics for average in {label}\")\n",
    "    mue, rmse = np.zeros(1000), np.zeros(1000)\n",
    "    for i in range(1000):\n",
    "        subset = np.random.choice(len(exp_ddg), size=len(exp_ddg), replace=True)\n",
    "        mue[i] = np.mean(np.abs(openfe_ddg[subset] - exp_ddg[subset]))\n",
    "        rmse[i] = np.sqrt(np.mean((openfe_ddg[subset] - exp_ddg[subset])**2))\n",
    "    average_data[\"RMSE\"] = np.mean(rmse)\n",
    "    average_data[\"RMSE lower\"] = np.percentile(rmse, 2.5)\n",
    "    average_data[\"RMSE upper\"] = np.percentile(rmse, 97.5)\n",
    "    average_data[\"MUE\"] = np.mean(mue)\n",
    "    average_data[\"MUE lower\"] = np.percentile(mue, 2.5)\n",
    "    average_data[\"MUE upper\"] = np.percentile(mue, 97.5)\n",
    "    pair_wise_stats.append(average_data)\n",
    "pair_wise_stats = pd.DataFrame(pair_wise_stats)\n",
    "pair_wise_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the public and private datasets which follow the Hahn benchmarking convention, 16 ligands and an exp range of 3 kcal/mol\n",
    "public_hahn_datasets, private_hahn_datasets = [], []\n",
    "# do public first\n",
    "for system in per_repeat_dgs_public[\"system group\"].unique():\n",
    "    system_df = per_repeat_dgs_public[per_repeat_dgs_public[\"system group\"] == system].copy(deep=True).reset_index(drop=True)\n",
    "    targets = system_df[\"system name\"].unique()\n",
    "    for target in targets:\n",
    "        target_df = system_df[(system_df[\"system name\"] == target)].copy(deep=True).reset_index(drop=True)\n",
    "        if len(target_df) >= 16 and target_df[\"exp DG (kcal/mol)\"].max() - target_df[\"exp DG (kcal/mol)\"].min() >= 3:\n",
    "            public_hahn_datasets.append((system, target))\n",
    "# do the same for the private data\n",
    "for system in per_repeat_dgs_private[\"system group\"].unique():\n",
    "    system_df = per_repeat_dgs_private[per_repeat_dgs_private[\"system group\"] == system].copy(deep=True).reset_index(drop=True)\n",
    "    targets = system_df[\"system name\"].unique()\n",
    "    for target in targets:\n",
    "        target_df = system_df[(system_df[\"system name\"] == target)].copy(deep=True).reset_index(drop=True)\n",
    "        if len(target_df) >= 16 and target_df[\"exp DG (kcal/mol)\"].max() - target_df[\"exp DG (kcal/mol)\"].min() >= 3:\n",
    "            private_hahn_datasets.append((system, target))\n",
    "print(\"Public Hahn datasets:\", public_hahn_datasets)\n",
    "print(\"Private Hahn datasets:\", private_hahn_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6648012",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(public_hahn_datasets), len(private_hahn_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the kendall tau ranking for the DGs for the public and private datasets only for datasets that follow the Hahn benchmarking convention\n",
    "ktau_data = []\n",
    "for dataset, label, valid_systems in [(per_repeat_dgs_public, \"Public\", public_hahn_datasets), (per_repeat_dgs_private, \"Private\", private_hahn_datasets)]:\n",
    "    for system, target in valid_systems:\n",
    "        target_df = dataset[(dataset[\"system group\"] == system) & (dataset[\"system name\"] == target) & (dataset[\"exp DG (kcal/mol)\"].notna())].copy(deep=True).reset_index(drop=True)\n",
    "        # calculate the kendall tau for each repeat and the average\n",
    "        for i in range(3):\n",
    "            # get the repeat DG values\n",
    "            repeat_dg = target_df[f\"repeat_{i}_DG (kcal/mol)\"].values\n",
    "            exp_dg = target_df[\"exp DG (kcal/mol)\"].values\n",
    "            s = stats.bootstrap_statistic(\n",
    "                y_pred=repeat_dg, y_true=exp_dg, statistic=\"KTAU\", nbootstrap=1000, ci=0.95\n",
    "            )\n",
    "            ktau_data.append({\n",
    "                \"system group\": system,\n",
    "                \"system name\": target,\n",
    "                \"repeat\": i + 1,\n",
    "                \"dataset\": label,\n",
    "                \"Kendall Tau\": s[\"mle\"],\n",
    "                \"Kendall Tau lower\": s[\"low\"],\n",
    "                \"Kendall Tau upper\": s[\"high\"],\n",
    "                \"N_ligs\": len(target_df),\n",
    "            })\n",
    "            \n",
    "        # calculate the kendall tau for the average\n",
    "        avg_dg = target_df[\"average DG (kcal/mol)\"].values\n",
    "        avg_exp_dg = target_df[\"exp DG (kcal/mol)\"].values\n",
    "        s = stats.bootstrap_statistic(\n",
    "            y_pred=avg_dg, y_true=avg_exp_dg, statistic=\"KTAU\", nbootstrap=1000, ci=0.95\n",
    "        )\n",
    "        ktau_data.append({\n",
    "            \"system group\": system,\n",
    "            \"system name\": target,\n",
    "            \"repeat\": \"average\",\n",
    "            \"dataset\": label,\n",
    "            \"Kendall Tau\": s[\"mle\"],\n",
    "            \"Kendall Tau lower\": s[\"low\"],\n",
    "            \"Kendall Tau upper\": s[\"high\"],\n",
    "            \"N_ligs\": len(target_df),\n",
    "        })\n",
    "ktau_data = pd.DataFrame(ktau_data)\n",
    "ktau_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079fa89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the public and private datasets calculate the weighted Ktau using the number of ligands as the weight\n",
    "public_ktau = ktau_data[ktau_data[\"dataset\"] == \"Public\"].copy(deep=True).reset_index(drop=True)\n",
    "private_ktau = ktau_data[ktau_data[\"dataset\"] == \"Private\"].copy(deep=True).reset_index(drop=True)\n",
    "# work out the weighted Ktau for each repeat and the average\n",
    "tau_table = []\n",
    "for i in range(3):\n",
    "    for dataset, label in [(public_ktau, \"Public\"), (private_ktau, \"Private\")]:\n",
    "        repeat_data = dataset[dataset[\"repeat\"] == i + 1].copy(deep=True).reset_index(drop=True)\n",
    "        weighted_tau = (repeat_data[\"Kendall Tau\"] * repeat_data[\"N_ligs\"]).sum() / repeat_data[\"N_ligs\"].sum()\n",
    "        # get the lower and upper bounds using bootstrapping over the systems\n",
    "        tau_bootstrap = []\n",
    "        for _ in range(1000):\n",
    "            sample = repeat_data.sample(n=len(repeat_data), replace=True)\n",
    "            weighted_tau_sample = (sample[\"Kendall Tau\"] * sample[\"N_ligs\"]).sum() / sample[\"N_ligs\"].sum()\n",
    "            tau_bootstrap.append(weighted_tau_sample)\n",
    "        weighted_tau_lower = np.percentile(tau_bootstrap, 2.5)\n",
    "        weighted_tau_upper = np.percentile(tau_bootstrap, 97.5)\n",
    "        tau_table.append({\n",
    "            \"dataset\": label,\n",
    "            \"repeat\": i + 1,\n",
    "            \"weighted Kendall Tau\": weighted_tau,\n",
    "            \"weighted Kendall Tau lower\": weighted_tau_lower,\n",
    "            \"weighted Kendall Tau upper\": weighted_tau_upper,\n",
    "        })\n",
    "# do the same for the average\n",
    "for dataset, label in [(public_ktau, \"Public\"), (private_ktau, \"Private\")]:\n",
    "    repeat_data = dataset[dataset[\"repeat\"] == \"average\"].copy(deep=True).reset_index(drop=True)\n",
    "    weighted_tau = (repeat_data[\"Kendall Tau\"] * repeat_data[\"N_ligs\"]).sum() / repeat_data[\"N_ligs\"].sum()\n",
    "    # get the lower and upper bounds using bootstrapping over the systems\n",
    "    tau_bootstrap = []\n",
    "    for _ in range(1000):\n",
    "        sample = repeat_data.sample(n=len(repeat_data), replace=True)\n",
    "        weighted_tau_sample = (sample[\"Kendall Tau\"] * sample[\"N_ligs\"]).sum() / sample[\"N_ligs\"].sum()\n",
    "        tau_bootstrap.append(weighted_tau_sample)\n",
    "    weighted_tau_lower = np.percentile(tau_bootstrap, 2.5)\n",
    "    weighted_tau_upper = np.percentile(tau_bootstrap, 97.5)\n",
    "    tau_table.append({\n",
    "        \"dataset\": label,\n",
    "        \"repeat\": \"average\",\n",
    "        \"weighted Kendall Tau\": weighted_tau,\n",
    "        \"weighted Kendall Tau lower\": weighted_tau_lower,\n",
    "        \"weighted Kendall Tau upper\": weighted_tau_upper,\n",
    "    })\n",
    "tau_table = pd.DataFrame(tau_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38268795",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ab925",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = wilcoxon(public_ktau[public_ktau[\"repeat\"] == \"average\"][\"Kendall Tau\"].values, public_ktau[public_ktau[\"repeat\"] == 3][\"Kendall Tau\"].values)\n",
    "print(f\"Wilcoxon test for public average vs repeat 1: statistic={stat}, p-value={p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717be213",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_repeat_dgs_public[(per_repeat_dgs_public[\"system group\"] == \"jacs_set\") & (per_repeat_dgs_public[\"system name\"] == \"ptp1b\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1bcf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptp1b_public_edges = normal_edge_data[(normal_edge_data[\"system group\"] == \"jacs_set\") & (normal_edge_data[\"system name\"] == \"ptp1b\")].copy(deep=True).reset_index(drop=True)\n",
    "# calculate the ddgs for repeat 2\n",
    "complex_dg = ptp1b_public_edges[\"complex_repeat_2_DG (kcal/mol)\"].values\n",
    "solvent_dg = ptp1b_public_edges[\"solvent_repeat_2_DG (kcal/mol)\"].values\n",
    "ptp1b_public_edges[[\"complex_repeat_2_DG (kcal/mol)\", \"solvent_repeat_2_DG (kcal/mol)\", \"failed\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_ktau[public_ktau[\"repeat\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f5a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "# perform a wilcoxon signed-rank test for the public and private datasets compairing the pairwise predictions between the repeats and the average\n",
    "def wilcoxon_signed_rank_test(df, repeat):\n",
    "    \"do the test and return a plot for the given repeat\"\n",
    "    # remove entries with nans\n",
    "    temp_df = df[(df[f\"repeat_{repeat} DDG (kcal/mol)\"].notna()) & (df[\"average DDG (kcal/mol)\"].notna())].copy(deep=True).reset_index(drop=True)\n",
    "    average_ddg = temp_df[\"average DDG (kcal/mol)\"].values\n",
    "    repeat_ddg = temp_df[f\"repeat_{repeat} DDG (kcal/mol)\"].values\n",
    "    # perform the wilcoxon signed-rank test\n",
    "    stat, p_value = wilcoxon(average_ddg, repeat_ddg)\n",
    "    print(f\"Wilcoxon signed-rank test for repeat {repeat}: statistic={stat}, p-value={p_value}\")\n",
    "    # create a plot of the differences\n",
    "    # calculate the bootstrap confidence interval for the difference between 4 ns and 5 ns\n",
    "    diffs = repeat_ddg - average_ddg\n",
    "    n_bootstrap = 1000\n",
    "    boot_diffs = []\n",
    "    n_edges = len(diffs)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(diffs, size=n_edges, replace=True)\n",
    "        boot_diffs.append(np.mean(sample))\n",
    "\n",
    "    ci_lower = np.percentile(boot_diffs, 2.5)\n",
    "    ci_upper = np.percentile(boot_diffs, 97.5)\n",
    "    mean_diff = np.mean(diffs)\n",
    "    # Plot: repeat X vs Average predictions and difference histogram\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Scatter plot of predictions\n",
    "    axs[0].scatter(average_ddg, repeat_ddg, color='#009384', edgecolor='k')\n",
    "    # workout the limits of the axes\n",
    "    min_val = min(np.min(average_ddg), np.min(repeat_ddg)) - 1\n",
    "    max_val = max(np.max(average_ddg), np.max(repeat_ddg)) + 1\n",
    "    axs[0].plot([min_val, max_val], [min_val, max_val], 'k--', label='y = x')\n",
    "    axs[0].set_xlabel(r'Average ââG$_{calc}$ (kcal/mol)', fontdict={\"fontsize\": 12})\n",
    "    axs[0].set_ylabel(r'Repeat ââG$_{calc}$ (kcal/mol)', fontdict={\"fontsize\": 12})\n",
    "    # set the size of the xticks on the x axis\n",
    "    axs[0].tick_params(axis='x', labelsize=12)\n",
    "    # same for the y axis\n",
    "    axs[0].tick_params(axis='y', labelsize=12)\n",
    "    # axs[0].xticks(fontsize=12)\n",
    "    axs[0].set_title(f'Comparison of Predictions for Repeat {repeat + 1} vs Average', fontdict={\"fontsize\": 14})\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    # plt.xticks(fontsize=12)\n",
    "\n",
    "    # Histogram of differences\n",
    "    sns.histplot(diffs, bins=15, kde=True, ax=axs[1], color='#009384')\n",
    "    axs[1].axvline(ci_lower, color='red', linestyle='--', label=f\"95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\", linewidth=2)\n",
    "    axs[1].axvline(ci_upper, color='red', linestyle='--', linewidth=2)\n",
    "    axs[1].axvline(mean_diff, color='black', linestyle='-', label=f\"Mean diff: {mean_diff:.3f}\")\n",
    "    axs[1].set_xlabel(f'ââG (Repeat {repeat} - Average)', fontdict={\"fontsize\": 12})\n",
    "    axs[1].set_ylabel('Frequency', fontdict={\"fontsize\": 12})\n",
    "    axs[1].set_title('Distribution of Prediction Differences', fontdict={\"fontsize\": 14})\n",
    "    axs[1].tick_params(axis='x', labelsize=12)\n",
    "    # same for the y axis\n",
    "    axs[1].tick_params(axis='y', labelsize=12)\n",
    "    axs[1].legend(fontsize=12)\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilcoxon_signed_rank_test(public_pariwise_diffs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilcoxon_signed_rank_test(private_pairwise_diffs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the absolute experimental DDG values in 1 kcal/mol bins calculate the probability of gettting the sign of the DDG correct with openfe and fep+ for each bin\n",
    "def bin_sign_correctness(data, bin_size=1.0):\n",
    "    \"\"\"\n",
    "    Bin the absolute experimental DDG values and calculate the probability of getting the sign correct.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame with 'Exp DDG (kcal/mol)', 'OpenFE DDG (kcal/mol)', 'FEP+ DDG (kcal/mol)' columns.\n",
    "    - bin_size: Size of the bins for absolute DDG values.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with bins and probabilities for OpenFE and FEP+.\n",
    "    \"\"\"\n",
    "    # Create bins\n",
    "    # bins = np.arange(0, 5 + bin_size, bin_size)\n",
    "    temp_df = data[data['Exp DDG (kcal/mol)'].notna()].copy(deep=True).reset_index(drop=True)\n",
    "    temp_df['abs_exp_ddg'] = np.abs(temp_df['Exp DDG (kcal/mol)'])\n",
    "    # max value\n",
    "    max_value = temp_df['abs_exp_ddg'].max()\n",
    "    bins = np.arange(0, max_value + bin_size, bin_size)\n",
    "    \n",
    "    # Bin the data\n",
    "    temp_df['bin'] = pd.cut(temp_df['abs_exp_ddg'], bins=bins, right=False)\n",
    "\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    results = []\n",
    "    # x = 0.5\n",
    "    # sort the bins by the lower edge\n",
    "    for b in sorted(temp_df['bin'].unique(), key=lambda x: x.left):\n",
    "        subset = temp_df[temp_df['bin'] == b]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "        # Calculate the probability of correct sign for the average and each repeat\n",
    "\n",
    "        average_correct = np.sum(np.sign(subset['average DDG (kcal/mol)']) == np.sign(subset['Exp DDG (kcal/mol)']))\n",
    "        repeat_1_correct = np.sum(np.sign(subset['repeat_0 DDG (kcal/mol)']) == np.sign(subset['Exp DDG (kcal/mol)']))\n",
    "        repeat_2_correct = np.sum(np.sign(subset['repeat_1 DDG (kcal/mol)']) == np.sign(subset['Exp DDG (kcal/mol)']))\n",
    "        repeat_3_correct = np.sum(np.sign(subset['repeat_2 DDG (kcal/mol)']) == np.sign(subset['Exp DDG (kcal/mol)']))\n",
    "        total = len(subset)\n",
    "        average_prob = average_correct / total\n",
    "        repeat_1_prob = repeat_1_correct / total\n",
    "        repeat_2_prob = repeat_2_correct / total\n",
    "        repeat_3_prob = repeat_3_correct / total\n",
    "        # Bootstrap the probabilities\n",
    "        nboots = 1000\n",
    "        average_probs = []\n",
    "        repeat_1_probs = []\n",
    "        repeat_2_probs = []\n",
    "        repeat_3_probs = []\n",
    "        for _ in range(nboots):\n",
    "            bootstrapped_subset = subset.sample(n=len(subset), replace=True)\n",
    "            average_correct_boot = np.sum(np.sign(bootstrapped_subset['average DDG (kcal/mol)']) == np.sign(bootstrapped_subset['Exp DDG (kcal/mol)']))\n",
    "            repeat_1_correct_boot = np.sum(np.sign(bootstrapped_subset['repeat_0 DDG (kcal/mol)']) == np.sign(bootstrapped_subset['Exp DDG (kcal/mol)']))\n",
    "            repeat_2_correct_boot = np.sum(np.sign(bootstrapped_subset['repeat_1 DDG (kcal/mol)']) == np.sign(bootstrapped_subset['Exp DDG (kcal/mol)']))\n",
    "            repeat_3_correct_boot = np.sum(np.sign(bootstrapped_subset['repeat_2 DDG (kcal/mol)']) == np.sign(bootstrapped_subset['Exp DDG (kcal/mol)']))\n",
    "            total_boot = len(bootstrapped_subset)\n",
    "            average_probs.append(average_correct_boot / total_boot)\n",
    "            repeat_1_probs.append(repeat_1_correct_boot / total_boot)\n",
    "            repeat_2_probs.append(repeat_2_correct_boot / total_boot)\n",
    "            repeat_3_probs.append(repeat_3_correct_boot / total_boot)\n",
    "        # Calculate mean and 95% CI\n",
    "        average_mean = np.mean(average_probs)\n",
    "        average_low = np.percentile(average_probs, 2.5)\n",
    "        average_high = np.percentile(average_probs, 97.5)\n",
    "        repeat_1_mean = np.mean(repeat_1_probs)\n",
    "        repeat_1_low = np.percentile(repeat_1_probs, 2.5)\n",
    "        repeat_1_high = np.percentile(repeat_1_probs, 97.5)\n",
    "        repeat_2_mean = np.mean(repeat_2_probs)     \n",
    "        repeat_2_low = np.percentile(repeat_2_probs, 2.5)\n",
    "        repeat_2_high = np.percentile(repeat_2_probs, 97.5)\n",
    "        repeat_3_mean = np.mean(repeat_3_probs)\n",
    "        repeat_3_low = np.percentile(repeat_3_probs, 2.5)\n",
    "        repeat_3_high = np.percentile(repeat_3_probs, 97.5)\n",
    "        # store the results so we can use hue to split the data\n",
    "        results.append({\n",
    "            'bin': b,\n",
    "            'Average Probability': average_mean,\n",
    "            'Average Probability Lower': average_low,\n",
    "            'Average Probability Upper': average_high,\n",
    "            'Repeat 1 Probability': repeat_1_mean,\n",
    "            'Repeat 1 Probability Lower': repeat_1_low,\n",
    "            'Repeat 1 Probability Upper': repeat_1_high,\n",
    "            'Repeat 2 Probability': repeat_2_mean,\n",
    "            'Repeat 2 Probability Lower': repeat_2_low,\n",
    "            'Repeat 2 Probability Upper': repeat_2_high,\n",
    "            'Repeat 3 Probability': repeat_3_mean,\n",
    "            'Repeat 3 Probability Lower': repeat_3_low,\n",
    "            'Repeat 3 Probability Upper': repeat_3_high,\n",
    "        })\n",
    "        # x+=1\n",
    "    return pd.DataFrame(results)\n",
    "# Calculate the binned probabilities\n",
    "binned_probabilities = bin_sign_correctness(public_pariwise_diffs, bin_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84febed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_probabilities_private = bin_sign_correctness(private_pairwise_diffs, bin_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the binned probabilities using a bar plot with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colours = sns.color_palette(\"rocket\", 4)\n",
    "# for i in range(3):\n",
    "#     sns.barplot(data=binned_probabilities, x='bin', y=f'Repeat {i+1} Probability', ax=ax,  color=colours[i], label=f'Repeat {i+1}', alpha=0.3, width=1.0)\n",
    "# sns.barplot(data=binned_probabilities, x='bin', y='Average Probability', ax=ax,  color=colours[3], label='Average', alpha=0.7, width=1.0)\n",
    "# add error bars for the probabilities\n",
    "ax.set_xlabel(r\"|$\\Delta\\Delta$G$_{exp}$| (kcal/mol)\", fontsize=14)\n",
    "ax.set_ylabel(r\"Probability Correct $\\Delta\\Delta$G$_{calc}$ Sign\", fontsize=14)\n",
    "# # set the x ticks to go from 0 to the max bin value in 1 kcal/mol increments\n",
    "# x_ticks = np.arange(1.5, 15.5, 2)\n",
    "# ax.errorbar(binned_probabilities['bin'] *2, binned_probabilities['OpenFE Probability'],\n",
    "#             yerr=[binned_probabilities['OpenFE Probability'] - binned_probabilities['OpenFE Probability Lower'],\n",
    "#                   binned_probabilities['OpenFE Probability Upper'] - binned_probabilities['OpenFE Probability']],\n",
    "#             fmt='none', color='black', capsize=5)\n",
    "# ax.errorbar(binned_probabilities['bin'] *2, binned_probabilities['FEP+ Probability'],\n",
    "#             yerr=[binned_probabilities['FEP+ Probability'] - binned_probabilities['FEP+ Probability Lower'],\n",
    "#                   binned_probabilities['FEP+ Probability Upper'] - binned_probabilities['FEP+ Probability']],\n",
    "#             fmt='none', color='black', capsize=5)\n",
    "# # add scatter points for the probabilities\n",
    "# get the lower half of the bin edges for the x ticks\n",
    "ticks = np.array([v.left for v in binned_probabilities['bin'].unique()] )\n",
    "# the center of each bin is the left edge + 0.25\n",
    "\n",
    "for i in range(3):\n",
    "    ax.plot(\n",
    "        ticks + 0.1 * (i + 1), \n",
    "        binned_probabilities[f'Repeat {i+1} Probability'], \n",
    "        color=colours[i], label=f'Repeat {i+1}', linewidth=2, marker='o', markersize=7,\n",
    "        markeredgecolor='black', markerfacecolor=colours[i], linestyle='--'\n",
    "    )\n",
    "ax.plot(\n",
    "    ticks + 0.4, \n",
    "    binned_probabilities['Average Probability'], \n",
    "    color=colours[3], markeredgecolor='black', label='Average', linewidth=2, marker='o', markersize=7, linestyle='--', markerfacecolor=colours[3]\n",
    ")\n",
    "# add error bars for the probabilities\n",
    "ax.errorbar(\n",
    "    ticks + 0.1, \n",
    "    binned_probabilities['Repeat 1 Probability'], \n",
    "    yerr=[binned_probabilities['Repeat 1 Probability'] - binned_probabilities['Repeat 1 Probability Lower'],\n",
    "          binned_probabilities['Repeat 1 Probability Upper'] - binned_probabilities['Repeat 1 Probability']],\n",
    "    fmt='none', color=colours[0], capsize=5, elinewidth=2\n",
    ")\n",
    "ax.errorbar(\n",
    "    ticks + 0.2, \n",
    "    binned_probabilities['Repeat 2 Probability'], \n",
    "    yerr=[binned_probabilities['Repeat 2 Probability'] - binned_probabilities['Repeat 2 Probability Lower'],\n",
    "          binned_probabilities['Repeat 2 Probability Upper'] - binned_probabilities['Repeat 2 Probability']],\n",
    "    fmt='none', color=colours[1], capsize=5, elinewidth=2\n",
    ")\n",
    "ax.errorbar(\n",
    "    ticks + 0.3, \n",
    "    binned_probabilities['Repeat 3 Probability'], \n",
    "    yerr=[binned_probabilities['Repeat 3 Probability'] - binned_probabilities['Repeat 3 Probability Lower'],\n",
    "          binned_probabilities['Repeat 3 Probability Upper'] - binned_probabilities['Repeat 3 Probability']],\n",
    "    fmt='none', color=colours[2], capsize=5, elinewidth=2\n",
    ")\n",
    "ax.errorbar(\n",
    "    ticks + 0.4, \n",
    "    binned_probabilities['Average Probability'], \n",
    "    yerr=[binned_probabilities['Average Probability'] - binned_probabilities['Average Probability Lower'],\n",
    "          binned_probabilities['Average Probability Upper'] - binned_probabilities['Average Probability']],\n",
    "    fmt='none', color=colours[3], capsize=5, elinewidth=2\n",
    ")\n",
    "ax.set_xticks(ticks)\n",
    "# ax.set_xticklabels([1, 2, 3, 4, 5, 6, 7], fontsize=12)\n",
    "plt.xlim((0,7.5))\n",
    "# save the figure\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.ylim(bottom=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the binned probabilities using a bar plot with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colours = sns.color_palette(\"rocket\", 4)\n",
    "# for i in range(3):\n",
    "#     sns.barplot(data=binned_probabilities, x='bin', y=f'Repeat {i+1} Probability', ax=ax,  color=colours[i], label=f'Repeat {i+1}', alpha=0.3, width=1.0)\n",
    "# sns.barplot(data=binned_probabilities, x='bin', y='Average Probability', ax=ax,  color=colours[3], label='Average', alpha=0.7, width=1.0)\n",
    "# add error bars for the probabilities\n",
    "ax.set_xlabel(r\"|$\\Delta\\Delta$G$_{exp}$| (kcal/mol)\", fontsize=14)\n",
    "ax.set_ylabel(r\"Probability Correct $\\Delta\\Delta$G$_{calc}$ Sign\", fontsize=14)\n",
    "# # set the x ticks to go from 0 to the max bin value in 1 kcal/mol increments\n",
    "# x_ticks = np.arange(1.5, 15.5, 2)\n",
    "# ax.errorbar(binned_probabilities['bin'] *2, binned_probabilities['OpenFE Probability'],\n",
    "#             yerr=[binned_probabilities['OpenFE Probability'] - binned_probabilities['OpenFE Probability Lower'],\n",
    "#                   binned_probabilities['OpenFE Probability Upper'] - binned_probabilities['OpenFE Probability']],\n",
    "#             fmt='none', color='black', capsize=5)\n",
    "# ax.errorbar(binned_probabilities['bin'] *2, binned_probabilities['FEP+ Probability'],\n",
    "#             yerr=[binned_probabilities['FEP+ Probability'] - binned_probabilities['FEP+ Probability Lower'],\n",
    "#                   binned_probabilities['FEP+ Probability Upper'] - binned_probabilities['FEP+ Probability']],\n",
    "#             fmt='none', color='black', capsize=5)\n",
    "# # add scatter points for the probabilities\n",
    "# get the lower half of the bin edges for the x ticks\n",
    "ticks = np.array([v.left for v in binned_probabilities_private['bin'].unique()] )\n",
    "# the center of each bin is the left edge + 0.25\n",
    "\n",
    "for i in range(3):\n",
    "    ax.plot(\n",
    "        ticks + 0.1 * (i + 1), \n",
    "        binned_probabilities_private[f'Repeat {i+1} Probability'], \n",
    "        color=colours[i], label=f'Repeat {i+1}', linewidth=2, marker='o', markersize=7,\n",
    "        markeredgecolor='black', markerfacecolor=colours[i], linestyle='--'\n",
    "    )\n",
    "ax.plot(\n",
    "    ticks + 0.4, \n",
    "    binned_probabilities_private['Average Probability'], \n",
    "    color=colours[3], markeredgecolor='black', label='Average', linewidth=2, marker='o', markersize=7, linestyle='--', markerfacecolor=colours[3]\n",
    ")\n",
    "# add error bars for the probabilities\n",
    "ax.errorbar(\n",
    "    ticks + 0.1, \n",
    "    binned_probabilities_private['Repeat 1 Probability'], \n",
    "    yerr=[binned_probabilities_private['Repeat 1 Probability'] - binned_probabilities_private['Repeat 1 Probability Lower'],\n",
    "          binned_probabilities_private['Repeat 1 Probability Upper'] - binned_probabilities_private['Repeat 1 Probability']],\n",
    "    fmt='none', color=colours[0], capsize=5, elinewidth=2\n",
    ")\n",
    "ax.errorbar(\n",
    "    ticks + 0.2, \n",
    "    binned_probabilities_private['Repeat 2 Probability'], \n",
    "    yerr=[binned_probabilities_private['Repeat 2 Probability'] - binned_probabilities_private['Repeat 2 Probability Lower'],\n",
    "          binned_probabilities_private['Repeat 2 Probability Upper'] - binned_probabilities_private['Repeat 2 Probability']],\n",
    "    fmt='none', color=colours[1], capsize=5, elinewidth=2\n",
    ")\n",
    "ax.errorbar(\n",
    "    ticks + 0.3, \n",
    "    binned_probabilities_private['Repeat 3 Probability'], \n",
    "    yerr=[binned_probabilities_private['Repeat 3 Probability'] - binned_probabilities_private['Repeat 3 Probability Lower'],\n",
    "          binned_probabilities_private['Repeat 3 Probability Upper'] - binned_probabilities_private['Repeat 3 Probability']],\n",
    "    fmt='none', color=colours[2], capsize=5, elinewidth=2\n",
    ")\n",
    "ax.errorbar(\n",
    "    ticks + 0.4, \n",
    "    binned_probabilities_private['Average Probability'], \n",
    "    yerr=[binned_probabilities_private['Average Probability'] - binned_probabilities_private['Average Probability Lower'],\n",
    "          binned_probabilities_private['Average Probability Upper'] - binned_probabilities_private['Average Probability']],\n",
    "    fmt='none', color=colours[3], capsize=5, elinewidth=2\n",
    ")\n",
    "ax.set_xticks(ticks)\n",
    "# ax.set_xticklabels([1, 2, 3, 4, 5, 6, 7], fontsize=12)\n",
    "# plt.xlim())\n",
    "# save the figure\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.ylim(bottom=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling reproducibility analysis\n",
    "from cinnabar import FEMap\n",
    "from openff.units import unit\n",
    "from scipy.stats import kendalltau\n",
    "def calculate_repeat_distribution(edge_df, dg_df, public=True) -> tuple[pd.DataFrame]:\n",
    "    if public:\n",
    "        group_name = \"system group\"\n",
    "        system_name = \"system name\"\n",
    "    else:\n",
    "        group_name = \"partner_id\"\n",
    "        system_name = \"dataset_name\"\n",
    "    nboots = 1000\n",
    "    pairwise_rmses, weighted_kendall_tau = np.zeros(nboots), np.zeros(nboots)\n",
    "    for i in range(nboots):\n",
    "        print(f\"Calculating repeat distribution for boot {i + 1} of {nboots}\")\n",
    "        calculated_dgs = []\n",
    "        for system in edge_df[group_name].unique():\n",
    "            # get the edges for this system\n",
    "            system_df = edge_df[edge_df[group_name] == system].copy(deep=True).reset_index(drop=True)\n",
    "            targets = system_df[system_name].unique()\n",
    "            for target in targets:\n",
    "                # get the edges for this target\n",
    "                target_df = system_df[(system_df[system_name] == target)].copy(deep=True).reset_index(drop=True)\n",
    "                exp_target_data = dg_df[(dg_df[system_name] == target) & (dg_df[group_name] == system)].copy(deep=True).reset_index(drop=True)\n",
    "                exp_shift = exp_target_data[\"Exp DG (kcal/mol)\"].mean()  # shift the experimental values to match the OpenFE values\n",
    "                # calculate the stats for each repeat\n",
    "                target_data = {}\n",
    "                # generate a list of edges randomly sampled from each repeat\n",
    "                complex_repeat_index = np.random.choice([0, 1, 2], size=len(target_df), replace=True)\n",
    "                solvent_repeat_index = np.random.choice([0, 1, 2], size=len(target_df), replace=True)\n",
    "                \n",
    "                # add the DDG estimates to the graph\n",
    "                fe_map = FEMap()\n",
    "                for index, row in target_df.iterrows():\n",
    "                    complex_repeat_id = complex_repeat_index[index]\n",
    "                    solent_repeat_id = solvent_repeat_index[index]\n",
    "                    complex_dg = row[f\"complex_repeat_{complex_repeat_id}_DG (kcal/mol)\"]\n",
    "                    complex_error = row[f\"complex_repeat_{complex_repeat_id}_dDG (kcal/mol)\"]\n",
    "                    solvent_dg = row[f\"solvent_repeat_{solent_repeat_id}_DG (kcal/mol)\"]\n",
    "                    solvent_error = row[f\"solvent_repeat_{solent_repeat_id}_dDG (kcal/mol)\"]\n",
    "                    uncertainty = (complex_error**2 + solvent_error**2)**0.5 * unit.kilocalorie_per_mole\n",
    "                    if uncertainty < 0.01 * unit.kilocalorie_per_mole:\n",
    "                        uncertainty = 0.1 * unit.kilocalorie_per_mole\n",
    "                    fe_map.add_relative_calculation(\n",
    "                        value=(complex_dg - solvent_dg) * unit.kilocalorie_per_mole,\n",
    "                        uncertainty=uncertainty if np.isfinite(uncertainty) else 0.1 * unit.kilocalorie_per_mole,\n",
    "                        labelA=row[\"ligand_A\"],\n",
    "                        labelB=row[\"ligand_B\"],\n",
    "                    )\n",
    "                # calculate the absolute DG values\n",
    "                fe_map.generate_absolute_values()\n",
    "                # get the absolute DG values\n",
    "                abs_df = fe_map.get_absolute_dataframe()\n",
    "                # write them to the target data\n",
    "                for _, abs_row in abs_df.iterrows():\n",
    "                    if abs_row[\"label\"] not in target_data:\n",
    "                        target_data[abs_row[\"label\"]] = {\"system group\": system, \"system name\": target, \"ligand name\": abs_row[\"label\"]}\n",
    "                        # add the exp data and the average calculated dg\n",
    "                        try:\n",
    "                            avg_data = exp_target_data[exp_target_data[\"ligand name\"] == abs_row[\"label\"]].iloc[0]\n",
    "                        except IndexError as e:\n",
    "                                print(abs_row[\"label\"], \"not found in experimental data\")\n",
    "                                continue\n",
    "                        target_data[abs_row[\"label\"]][\"Exp DG (kcal/mol)\"] = avg_data[\"Exp DG (kcal/mol)\"]\n",
    "                        target_data[abs_row[\"label\"]][\"Exp dDG (kcal/mol)\"] = avg_data[\"Exp dDG (kcal/mol)\"]\n",
    "                    # add the repeat DG value and the uncertainty\n",
    "                    target_data[abs_row[\"label\"]][f\"repeat_DG (kcal/mol)\"] = abs_row[\"DG (kcal/mol)\"] + exp_shift\n",
    "                    target_data[abs_row[\"label\"]][f\"repeat_dDG (kcal/mol)\"] = abs_row[\"uncertainty (kcal/mol)\"]\n",
    "\n",
    "                calculated_dgs.extend(list(target_data.values()))\n",
    "        calculated_dgs = pd.DataFrame(calculated_dgs)\n",
    "        # calculate the weighted kendall tau for each of the hahn systems\n",
    "        kendall_tau = []\n",
    "        target_weights = []\n",
    "        for system in calculated_dgs[\"system group\"].unique():\n",
    "            system_df = calculated_dgs[calculated_dgs[\"system group\"] == system].copy(deep=True).reset_index(drop=True)\n",
    "            targets = system_df[\"system name\"].unique()\n",
    "            for target in targets:\n",
    "                target_df = system_df[(system_df[\"system name\"] == target) & (system_df[\"Exp DG (kcal/mol)\"].notna())].copy(deep=True).reset_index(drop=True)\n",
    "                if len(target_df) < 16 or target_df[\"Exp DG (kcal/mol)\"].max() - target_df[\"Exp DG (kcal/mol)\"].min() < 3:\n",
    "                    continue\n",
    "                repeat_dg = target_df[f\"repeat_DG (kcal/mol)\"].values\n",
    "                exp_dg = target_df[\"Exp DG (kcal/mol)\"].values\n",
    "                tau, _ = kendalltau(repeat_dg, exp_dg)\n",
    "                # add tau times the weight for this target\n",
    "                kendall_tau.append(tau * len(target_df))\n",
    "                target_weights.append(len(target_df))\n",
    "        # calculate the weighted kendall tau\n",
    "        weighted_kendall_tau[i] = np.sum(kendall_tau) / np.sum(target_weights)\n",
    "\n",
    "        # calculate all pairwise DDG differences\n",
    "        pairwise_diffs_calc, pairwise_diffs_exp = [], []\n",
    "        for system in calculated_dgs[\"system group\"].unique():\n",
    "            system_df = calculated_dgs[calculated_dgs[\"system group\"] == system].copy(deep=True).reset_index(drop=True)\n",
    "            targets = system_df[\"system name\"].unique()\n",
    "            for target in targets:\n",
    "                target_df = system_df[(system_df[\"system name\"] == target) & (system_df[\"Exp DG (kcal/mol)\"].notna())].copy(deep=True).reset_index(drop=True)\n",
    "                # calculate the pairwise differences for each repeat\n",
    "                unique_ligands = target_df[\"ligand name\"].unique()\n",
    "                for x, ligand_A in enumerate(unique_ligands):\n",
    "                    for y, ligand_B in enumerate(unique_ligands):\n",
    "                        if x >= y:\n",
    "                            continue\n",
    "                        # get the repeat DG values for each ligand\n",
    "                        repeat_dg_A = target_df[target_df[\"ligand name\"] == ligand_A][f\"repeat_DG (kcal/mol)\"].values[0]\n",
    "                        repeat_dg_B = target_df[target_df[\"ligand name\"] == ligand_B][f\"repeat_DG (kcal/mol)\"].values[0]\n",
    "                        exp_dg_A = target_df[target_df[\"ligand name\"] == ligand_A][\"Exp DG (kcal/mol)\"].values[0]\n",
    "                        exp_dg_B = target_df[target_df[\"ligand name\"] == ligand_B][\"Exp DG (kcal/mol)\"].values[0]  \n",
    "                        pairwise_diffs_calc.append((repeat_dg_B - repeat_dg_A))\n",
    "                        pairwise_diffs_exp.append((exp_dg_B - exp_dg_A))\n",
    "        # calculate the RMSE for the pairwise differences\n",
    "        pairwise_diffs_calc = np.array(pairwise_diffs_calc)\n",
    "        pairwise_diffs_exp = np.array(pairwise_diffs_exp)\n",
    "        pairwise_rmses[i] = np.sqrt(np.mean((pairwise_diffs_calc - pairwise_diffs_exp) ** 2))\n",
    "        print(f\"Boot {i + 1} - Pairwise RMSE: {pairwise_rmses[i]:.3f}, Weighted Kendall Tau: {weighted_kendall_tau[i]:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return pairwise_rmses, weighted_kendall_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3322c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_repeat_rmses, single_repeat_tau = calculate_repeat_distribution(normal_edge_data, public_dg_data, public=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7be35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(single_repeat_rmses, color=\"#009384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58164203",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(single_repeat_tau, color=\"#009384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a09011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same again but use averages combining 3 randomly selected repeats for each edge\n",
    "def calculate_average_distribution(edge_df, dg_df, public=True) -> tuple[pd.DataFrame]:\n",
    "    if public:\n",
    "        group_name = \"system group\"\n",
    "        system_name = \"system name\"\n",
    "    else:\n",
    "        group_name = \"partner_id\"\n",
    "        system_name = \"dataset_name\"\n",
    "    nboots = 1000\n",
    "    pairwise_rmses, weighted_kendall_tau = np.zeros(nboots), np.zeros(nboots)\n",
    "    for i in range(nboots):\n",
    "        print(f\"Calculating average distribution for boot {i + 1} of {nboots}\")\n",
    "        calculated_dgs = []\n",
    "        for system in edge_df[group_name].unique():\n",
    "            # get the edges for this system\n",
    "            system_df = edge_df[edge_df[group_name] == system].copy(deep=True).reset_index(drop=True)\n",
    "            targets = system_df[system_name].unique()\n",
    "            for target in targets:\n",
    "                # get the edges for this target\n",
    "                target_df = system_df[(system_df[system_name] == target)].copy(deep=True).reset_index(drop=True)\n",
    "                exp_target_data = dg_df[(dg_df[system_name] == target) & (dg_df[group_name] == system)].copy(deep=True).reset_index(drop=True)\n",
    "                exp_shift = exp_target_data[\"Exp DG (kcal/mol)\"].mean()  # shift the experimental values to match the OpenFE values\n",
    "                # calculate the stats for each repeat\n",
    "                target_data = {}\n",
    "                \n",
    "                # add the DDG estimates to the graph\n",
    "                fe_map = FEMap()\n",
    "                for index, row in target_df.iterrows():\n",
    "                    # randomly select 3 repeats\n",
    "                    complex_repeat_ids = np.random.choice([0, 1, 2], size=3, replace=True)\n",
    "                    solvent_repeat_ids = np.random.choice([0, 1, 2], size=3, replace=True)\n",
    "                    complex_dg = np.mean([row[f\"complex_repeat_{repeat_id}_DG (kcal/mol)\"] for repeat_id in complex_repeat_ids])\n",
    "                    complex_error = np.std([row[f\"complex_repeat_{repeat_id}_dDG (kcal/mol)\"] for repeat_id in complex_repeat_ids])\n",
    "                    solvent_dg = np.mean([row[f\"solvent_repeat_{repeat_id}_DG (kcal/mol)\"] for repeat_id in solvent_repeat_ids])\n",
    "                    solvent_error = np.std([row[f\"solvent_repeat_{repeat_id}_dDG (kcal/mol)\"] for repeat_id in solvent_repeat_ids])\n",
    "                    uncertainty = (complex_error**2 + solvent_error**2)**0.5 * unit.kilocalorie_per_mole\n",
    "                    if uncertainty < 0.01 * unit.kilocalorie_per_mole:\n",
    "                        uncertainty = 0.1 * unit.kilocalorie_per_mole\n",
    "                    fe_map.add_relative_calculation(\n",
    "                        value=(complex_dg - solvent_dg) * unit.kilocalorie_per_mole,\n",
    "                        uncertainty=uncertainty if np.isfinite(uncertainty) else 0.1 * unit.kilocalorie_per_mole,\n",
    "                        labelA=row[\"ligand_A\"],\n",
    "                        labelB=row[\"ligand_B\"],\n",
    "                    )\n",
    "                # calculate the absolute DG values\n",
    "                fe_map.generate_absolute_values()\n",
    "                # get the absolute DG values\n",
    "                abs_df = fe_map.get_absolute_dataframe()\n",
    "                # write them to the target data\n",
    "                for _, abs_row in abs_df.iterrows():\n",
    "                    if abs_row[\"label\"] not in target_data:\n",
    "                        target_data[abs_row[\"label\"]] = {\"system group\": system, \"system name\": target, \"ligand name\": abs_row[\"label\"]}\n",
    "                        # add the exp data and the average calculated dg\n",
    "                        try:\n",
    "                            avg_data = exp_target_data[exp_target_data[\"ligand name\"] == abs_row[\"label\"]].iloc[0]\n",
    "                        except IndexError as e:\n",
    "                                print(abs_row[\"label\"], \"not found in experimental data\")\n",
    "                                continue\n",
    "                        target_data[abs_row[\"label\"]][\"Exp DG (kcal/mol)\"] = avg_data[\"Exp DG (kcal/mol)\"]\n",
    "                        target_data[abs_row[\"label\"]][\"Exp dDG (kcal/mol)\"] = avg_data[\"Exp dDG (kcal/mol)\"]\n",
    "                    # add the repeat DG value and the uncertainty\n",
    "                    target_data[abs_row[\"label\"]][f\"average_DG (kcal/mol)\"] = abs_row[\"DG (kcal/mol)\"] + exp_shift\n",
    "                    target_data[abs_row[\"label\"]][f\"average_dDG (kcal/mol)\"] = abs_row[\"uncertainty (kcal/mol)\"]\n",
    "\n",
    "                calculated_dgs.extend(list(target_data.values()))\n",
    "        calculated_dgs = pd.DataFrame(calculated_dgs)\n",
    "        # calculate the weighted kendall tau for each of the hahn systems\n",
    "        kendall_tau = []\n",
    "        target_weights = []\n",
    "        for system in calculated_dgs[\"system group\"].unique():\n",
    "            system_df = calculated_dgs[calculated_dgs[\"system group\"] == system].copy(deep=True).reset_index(drop=True)\n",
    "            targets = system_df[\"system name\"].unique()\n",
    "            for target in targets:\n",
    "                target_df = system_df[(system_df[\"system name\"] == target) & (system_df[\"Exp DG (kcal/mol)\"].notna())].copy(deep=True).reset_index(drop=True)\n",
    "                if len(target_df) < 16 or target_df[\"Exp DG (kcal/mol)\"].max() - target_df[\"Exp DG (kcal/mol)\"].min() < 3:\n",
    "                    continue\n",
    "                repeat_dg = target_df[f\"average_DG (kcal/mol)\"].values\n",
    "                exp_dg = target_df[\"Exp DG (kcal/mol)\"].values\n",
    "                tau, _ = kendalltau(repeat_dg, exp_dg)\n",
    "                # add tau times the weight for this target\n",
    "                kendall_tau.append(tau * len(target_df))\n",
    "                target_weights.append(len(target_df))\n",
    "        # calculate the weighted kendall tau\n",
    "        weighted_kendall_tau[i] = np.sum(kendall_tau) / np.sum(target_weights)\n",
    "\n",
    "        # calculate all pairwise DDG differences\n",
    "        pairwise_diffs_calc, pairwise_diffs_exp = [], []\n",
    "        for system in calculated_dgs[\"system group\"].unique():\n",
    "            system_df = calculated_dgs[calculated_dgs[\"system group\"] == system].copy(deep=True).reset_index(drop=True)\n",
    "            targets = system_df[\"system name\"].unique()\n",
    "            for target in targets:\n",
    "                target_df = system_df[(system_df[\"system name\"] == target) & (system_df[\"Exp DG (kcal/mol)\"].notna())].copy(deep=True).reset_index(drop=True)\n",
    "                # calculate the pairwise differences for each repeat\n",
    "                unique_ligands = target_df[\"ligand name\"].unique()\n",
    "                for x, ligand_A in enumerate(unique_ligands):\n",
    "                    for y, ligand_B in enumerate(unique_ligands):\n",
    "                        if x >= y:\n",
    "                            continue\n",
    "                        # get the repeat DG values for each ligand\n",
    "                        repeat_dg_A = target_df[target_df[\"ligand name\"] == ligand_A][f\"average_DG (kcal/mol)\"].values[0]\n",
    "                        repeat_dg_B = target_df[target_df[\"ligand name\"] == ligand_B][f\"average_DG (kcal/mol)\"].values[0]\n",
    "                        exp_dg_A = target_df[target_df[\"ligand name\"] == ligand_A][\"Exp DG (kcal/mol)\"].values[0]\n",
    "                        exp_dg_B = target_df[target_df[\"ligand name\"] == ligand_B][\"Exp DG (kcal/mol)\"].values[0]  \n",
    "                        pairwise_diffs_calc.append((repeat_dg_B - repeat_dg_A))\n",
    "                        pairwise_diffs_exp.append((exp_dg_B - exp_dg_A))\n",
    "        # calculate the RMSE for the pairwise differences\n",
    "        pairwise_diffs_calc = np.array(pairwise_diffs_calc)\n",
    "        pairwise_diffs_exp = np.array(pairwise_diffs_exp)\n",
    "        pairwise_rmses[i] = np.sqrt(np.mean((pairwise_diffs_calc - pairwise_diffs_exp) ** 2))\n",
    "        print(f\"Boot {i + 1} - Pairwise RMSE: {pairwise_rmses[i]:.3f}, Weighted Kendall Tau: {weighted_kendall_tau[i]:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return pairwise_rmses, weighted_kendall_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_average_rmses, public_average_tau = calculate_average_distribution(normal_edge_data, public_dg_data, public=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace0216",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_average_rmses, private_average_tau = calculate_average_distribution(private_edge_data, private_dg_data, public=False)\n",
    "private_repeat_rmses, private_repeat_tau = calculate_repeat_distribution(private_edge_data, private_dg_data, public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a box plot of the pairwise RMSD values for the public and private datasets for the single and average repeat methods\n",
    "# make a new dataframe with the RMSD values and the method\n",
    "all_data = pd.DataFrame({\n",
    "    'RMSD': np.concatenate([single_repeat_rmses, public_average_rmses, private_repeat_rmses, private_average_rmses]),\n",
    "    'Method': ['Single'] * len(single_repeat_rmses) + ['Average'] * len(public_average_rmses) +\n",
    "              ['Single'] * len(private_repeat_rmses) + ['Average'] * len(private_average_rmses),\n",
    "    'Dataset': ['Public'] * (len(single_repeat_rmses) + len(public_average_rmses)) +\n",
    "               ['Private'] * (len(private_repeat_rmses) + len(private_average_rmses)),\n",
    "    \"Kendall Tau\": np.concatenate([single_repeat_tau, public_average_tau, private_repeat_tau, private_average_tau])\n",
    "})\n",
    "\n",
    "# Plot the RMSD values\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n",
    "sns.boxplot(data=all_data, x=\"Dataset\", hue=\"Method\", y='RMSD', ax=axs[0], palette=[\"#009384\", \"#F2A900\"])\n",
    "sns.boxplot(data=all_data, x=\"Dataset\", hue=\"Method\", y='Kendall Tau', ax=axs[1], palette=[\"#009384\", \"#F2A900\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "stat, p = wilcoxon(single_repeat_rmses, public_average_rmses)\n",
    "print(f\"Wilcoxon test for pairwise RMSD: stat={stat:.3f}, p-value={p:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = wilcoxon(single_repeat_tau, public_average_tau)\n",
    "print(f\"Wilcoxon test for weighted Kendall Tau: stat={stat:.3f}, p-value={p:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72619b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_diff = single_repeat_rmses - public_average_rmses\n",
    "stat, p = wilcoxon(rmse_diff, alternative=\"greater\")  # if you expect single > triplicate\n",
    "print(f\"Wilcoxon p = {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(rmse_diff), np.mean(rmse_diff), np.std(rmse_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c36e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_diff = single_repeat_tau - public_average_tau\n",
    "stat, p = wilcoxon(tau_diff, alternative=\"less\")  # if you expect single > triplicate\n",
    "print(f\"Wilcoxon p = {p:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e57225",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(tau_diff), np.mean(tau_diff), np.std(tau_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c6f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.choice([0, 1, 2], size=3, replace=True)\n",
    "if x[0] == x[1] and x[1] == x[2]:\n",
    "    print(\"All repeats are the same\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asapdiscovery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
